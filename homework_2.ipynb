{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework #2\n",
    "## Opal Issan. \n",
    "\n",
    "# Step Length selection - exact method. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "%matplotlib notebook\n",
    "plt.rcParams['figure.figsize'] = [10, 5] # default fig size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rosenbrock_fun(x):\n",
    "    x1, x2 = x\n",
    "    \"\"\" This function returns the output of the Rosenbrock function.\"\"\"\n",
    "    return 100 * ((x2 - x1 ** 2) ** 2) + (1 - x1) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rosenbrock_gradient(x):\n",
    "    x1, x2 = x\n",
    "    \"\"\" return [df/dx1 df/dx2]\"\"\"\n",
    "    dfx1 = -400 * x2 * x1 + 400 * (x1 ** 3) - 2 + 2 * x1\n",
    "    dfx2 = 200 * x2 - 200 * (x1 ** 2)\n",
    "    return np.array([dfx1, dfx2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rosenbrock_hessian(x):\n",
    "    \"\"\" return [d2f/dx1^2   d2f/dx1dx2\n",
    "                d2f/dx1dx2  d2f/dx2^2]\"\"\"\n",
    "    x1, x2 = x\n",
    "    h = np.zeros((2, 2))\n",
    "    h[0, 0] = -400*x2 + 1200*(x1**2) + 2\n",
    "    h[0, 1] = -400*x1\n",
    "    h[1, 0] = -400*x1\n",
    "    h[1, 1] = 200 \n",
    "    return h "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pk_steepest_descent(gradient):\n",
    "    \"\"\" search direction for steepest decent.\"\"\"\n",
    "    return np.array(-1*gradient/np.linalg.norm(gradient))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pk_newton(gradient, hessian):\n",
    "    \"\"\" search direction for Newton's method.\"\"\"\n",
    "    h_inv = np.linalg.inv(hessian)\n",
    "    return -np.matmul(h_inv, gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi_function(alpha, pk, xk):\n",
    "    \"\"\" phi(alpha) = f(xk + alpha*pk)\"\"\"\n",
    "    x = xk + alpha*pk \n",
    "    return rosenbrock_fun(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi_prime(pk, xk):\n",
    "    return np.dot(rosenbrock_gradient(xk), pk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hermite(alpha_0, alpha_1, pk, xk):\n",
    "    \"\"\"interpolate phi(a0), phi'(a0), phi(a1), phi'(a1)\"\"\"\n",
    "    d1 = phi_prime(pk, xk+alpha_0*pk) + phi_prime(pk, xk+alpha_1*pk) -3*\\\n",
    "            (phi_function(alpha_0, pk, xk)-phi_function(alpha_1, pk, xk))/(alpha_0 - alpha_1)\n",
    "    d2 = np.sign(alpha_1 - alpha_0)*np.sqrt(d1**2 - phi_prime(pk, xk+alpha_0*pk)*phi_prime(pk, xk+alpha_1*pk))\n",
    "    \n",
    "    frac = (phi_prime(pk, xk+alpha_1*pk) + d2 - d1)/\\\n",
    "            (phi_prime(pk, xk+alpha_1*pk) - phi_prime(pk, xk+alpha_0*pk) +2*d2)\n",
    "    \n",
    "    return alpha_1 - (alpha_1 - alpha_0)*frac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quadradic_interp(alpha_0, pk, xk):\n",
    "    \"\"\" interpolate over phi(0), phi'(0), phi(alpha_0)\"\"\"\n",
    "    top = (alpha_0 ** 2) * (phi_prime(pk, xk))\n",
    "    bottom = (phi_function(alpha_0, pk, xk) - phi_function(0, pk, xk) - alpha_0 * phi_prime(pk, xk))\n",
    "    return - top / (2 * bottom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cubic_interp(alpha_0, alpha_1, xk, pk):\n",
    "    # interpolate to the 3rd order \n",
    "    # over the points: phi(0), phi'(0), phi(alpha_0), phi(alpha_1)\n",
    "    # the cubic function is in this form:\n",
    "    # phi_c(alpha) = a*alpha^3 + b* alpha^2 + alpha*phi'(0)  + phi(0)\n",
    "    coeff = 1/((alpha_0**2)*(alpha_1**2)*(alpha_1 - alpha_0))\n",
    "    \n",
    "    mat_1 = np.zeros((2,2))\n",
    "    mat_1[0, 0] = alpha_0**2\n",
    "    mat_1[0, 1] = -alpha_1**2\n",
    "    mat_1[1, 0] = -alpha_0**3\n",
    "    mat_1[1, 1] = -alpha_1**3\n",
    "    \n",
    "    mat_2 = np.zeros(2)\n",
    "    mat_2[0] = phi_function(alpha_1, pk, xk) - phi_function(0, pk, xk) - alpha_1*phi_prime(pk, xk)\n",
    "    mat_2[1] = phi_function(alpha_0, pk, xk) - phi_function(0, pk, xk) - alpha_0*phi_prime(pk, xk)\n",
    "    ab_vec = coeff*np.matmul(mat_1, mat_2)\n",
    "    \n",
    "    a = ab_vec[0]\n",
    "    b = ab_vec[1]\n",
    "    \n",
    "    return (-b + np.sqrt(b**2 - 3*a*phi_prime(pk,xk)))/(3*a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolation_method(alpha_0, alpha_1, xk, pk, c1):\n",
    "    # compute quadradic interp min.\n",
    "    alpha_q = quadradic_interp(alpha_0, pk, xk)\n",
    "    \n",
    "    # check the armijo condition.\n",
    "    if phi_function(alpha_q, pk, xk) > phi_function(0, pk, xk) + c1*alpha_q*phi_prime(pk, xk):\n",
    "        \n",
    "        # create cubic interpolation \n",
    "        alpha_c1 = cubic_interp(alpha_0, alpha_q, xk, pk)\n",
    "        \n",
    "        # check the armijo condition. \n",
    "        if phi_function(alpha_c1, pk, xk) > phi_function(0, pk, xk) + c1*alpha_c1*phi_prime(pk, xk):\n",
    "            \n",
    "            # create cubic interpolation \n",
    "            alpha_c2 = cubic_interp(alpha_q, alpha_c1, xk, pk)\n",
    "            \n",
    "            # check the armijo condition.\n",
    "            if phi_function(alpha_c2, pk, xk) > phi_function(0, pk, xk) + c1*alpha_c2*phi_prime(pk, xk):\n",
    "                \n",
    "                # safeguard\n",
    "                return hermite(alpha_c1, alpha_c2, pk, xk)\n",
    "            else:\n",
    "                return alpha_c2\n",
    "        else:\n",
    "            return alpha_c1\n",
    "    else:\n",
    "        return alpha_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolation(alpha_0, alpha_1, xk, pk, c1):\n",
    "    # call hermite interpolation. \n",
    "    try:\n",
    "        alpha_star = hermite(alpha_0, alpha_1, pk, xk)\n",
    "        return alpha_star\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zoom(alpha_low, alpha_high, xk, pk, c1, c2):\n",
    "    \"\"\" find xj in the interval of alpha_low and alpha_high. \"\"\"\n",
    "    max_iter = 10\n",
    "    k = 0\n",
    "    while max_iter > k:\n",
    "        if alpha_low == alpha_high:\n",
    "            return None\n",
    "        if phi_function(alpha_high, pk, xk) < phi_function(alpha_low, pk, xk):\n",
    "            return None\n",
    "        if phi_prime(pk, xk + alpha_low * pk) * (alpha_high - alpha_low) >= 0:\n",
    "            return None\n",
    "\n",
    "        # interpolate to find xj between alpha_low and alpha_high\n",
    "        alpha_j = interpolation(alpha_0=alpha_low, alpha_1=alpha_high, xk=xk, pk=pk, c1=c1)\n",
    "\n",
    "        # if interpolation fails:\n",
    "        if alpha_j is None:\n",
    "            alpha_j = (alpha_high - alpha_low) / 2\n",
    "\n",
    "        # compute phi(xj)\n",
    "        res = phi_function(alpha_j, pk, xk)\n",
    "        # test the Armijo condition.\n",
    "        if (res > phi_function(0, pk, xk) + c1 * alpha_j * phi_prime(pk, xk)) or (\n",
    "                res >= phi_function(alpha_low, pk, xk)):\n",
    "            alpha_high = alpha_j\n",
    "\n",
    "        else:\n",
    "            # compute phi_prime(x_j)\n",
    "            if np.abs(phi_prime(pk, xk + alpha_j * pk)) <= -c2 * phi_prime(pk, xk):\n",
    "                # satisfy the curvature condition.\n",
    "                return alpha_j\n",
    "\n",
    "            if phi_prime(pk, xk + alpha_j * pk) * (alpha_high - alpha_low) >= 0:\n",
    "                alpha_high = alpha_low\n",
    "            alpha_low = alpha_j\n",
    "\n",
    "        k += 1\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_line_search(c1, c2, pk, xk, old_x=None, alpha_0=0, alpha_max=1, method=\"sd\"):\n",
    "    \"\"\"Find alpha that satisfies strong Wolfe conditions.\"\"\"\n",
    "    phi0 = phi_function(0, pk, xk)\n",
    "    dphi0 = phi_prime(pk, xk)\n",
    "\n",
    "    # choose alpha_1\n",
    "    if old_x is not None and dphi0 != 0 and method == \"sd\":\n",
    "        alpha_1 = min(1.0, 1.01 * 2 * (rosenbrock_fun(xk) - rosenbrock_fun(old_x)) / dphi0)\n",
    "    else:\n",
    "        alpha_1 = 1.0\n",
    "\n",
    "    if alpha_1 <= 0:\n",
    "        alpha_1 = 1.0\n",
    "\n",
    "    if alpha_max is not None:\n",
    "        alpha_1 = min(alpha_1, alpha_max)\n",
    "\n",
    "    alpha_vec = [alpha_0, alpha_1]\n",
    "\n",
    "    i = 1\n",
    "    while True:\n",
    "        # alpha i = ai\n",
    "        alpha_i = alpha_vec[i]\n",
    "        # compute phi(ai)\n",
    "        phi_i = phi_function(alpha_i, pk, xk)\n",
    "        # Armijo condition.\n",
    "        if phi_i > phi0 + c1 * alpha_i * dphi0 \\\n",
    "                or (i > 1 and phi_function(alpha_i, pk, xk) >= phi_function(alpha_vec[i - 1], pk, xk)):\n",
    "            return zoom(alpha_low=alpha_vec[i - 1], alpha_high=alpha_vec[i], xk=xk, pk=pk, c1=c1, c2=c2)\n",
    "\n",
    "        # compute phi prime at alpha i (ai).\n",
    "        phi_prime_alpha_i = phi_prime(pk, xk + alpha_i * pk)\n",
    "        # curvature condition.\n",
    "        if abs(phi_prime_alpha_i) <= -c2 * dphi0:\n",
    "            return alpha_i\n",
    "\n",
    "        if phi_prime_alpha_i >= 0:\n",
    "            return zoom(alpha_low=alpha_i, alpha_high=alpha_vec[i - 1], xk=xk, pk=pk, c1=c1, c2=c2)\n",
    "\n",
    "        alpha_vec.append(zoom(alpha_low=alpha_i, alpha_high=alpha_max, xk=xk, pk=pk, c1=c1, c2=c2))\n",
    "        i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_local_minimum(x0, c1, c2, alpha, p, tol=1e-8, print_num=None, method=\"sd\", save_xk=True):\n",
    "    \"\"\" Find the local minimum point x* using backtracking line search that will satisfy Armijo-Goldstein inequality.\n",
    "    The avilable methods: Newton and Steepest Descent. Default is Steepest descent.\n",
    "    x0 - initial guess for x*.\n",
    "    c1 - the slope of Armijo-Goldstein line.\n",
    "    alpha - initial step size.\n",
    "    p - modify alpha scaler.\n",
    "    tol - tolerence. the iterative method will stop when ||gradient|| < tol\"\"\"\n",
    "\n",
    "    xk = x0\n",
    "    k = 0  # iteration number\n",
    "    alpha_original = alpha\n",
    "    alpha_prev = 1\n",
    "\n",
    "    if save_xk:\n",
    "        xk_arr = np.array([xk])\n",
    "\n",
    "    while rosenbrock_fun(xk) > tol and np.linalg.norm(rosenbrock_gradient(xk)) > tol:\n",
    "        \"\"\" find the next iteration xk+1\"\"\"\n",
    "        gradient = rosenbrock_gradient(xk)\n",
    "\n",
    "        if method == \"sd\":\n",
    "            pk = pk_steepest_descent(gradient)\n",
    "\n",
    "        if method == \"newton\":\n",
    "            hessian = rosenbrock_hessian(xk)\n",
    "            pk = pk_newton(gradient, hessian)\n",
    "\n",
    "        if print_num is not None:\n",
    "            if 0 <= k <= 6:\n",
    "                if k == 0:\n",
    "                    print(\"***The first 6 iterations:*** \\n\")\n",
    "                print(\"Iteration #\" + str(k) + \", x\" + str(k) + \" = \" + str(xk))\n",
    "                print(\"||gradient|| = \" + str(np.linalg.norm(gradient)))\n",
    "                print(\"f = \" + str(rosenbrock_fun(xk)) + \"\\n\")\n",
    "\n",
    "            if print_num - 5 <= k <= print_num and k > 6:\n",
    "                if k == print_num - 5 or k == 7:\n",
    "                    print(\"***The last 6 iterations:*** \\n\")\n",
    "                print(\"Iteration #\" + str(k) + \", x\" + str(k) + \" = \" + str(xk))\n",
    "                print(\"||gradient|| = \" + str(np.linalg.norm(gradient)))\n",
    "                print(\"f = \" + str(rosenbrock_fun(xk)) + \"\\n\")\n",
    "\n",
    "        xk_next = xk + alpha * pk\n",
    "\n",
    "        while rosenbrock_fun(xk_next) > rosenbrock_fun(xk) + c1 * alpha * np.matmul(pk.T, gradient):\n",
    "            \"\"\" find a step size that will satisfy Armijo-Goldstein inequality. Modify alpha. \"\"\"\n",
    "            #print(\"call line search\")\n",
    "            if k > 1:\n",
    "                old_x = xk_arr[-4:-2]\n",
    "            else:\n",
    "                old_x = None\n",
    "            alpha = my_line_search(c1=c1, c2=c2, pk=pk, xk=xk, old_x=old_x, alpha_0=0, alpha_max=1, method=method)\n",
    "            # alpha = line_search(rosenbrock_fun, rosenbrock_gradient, xk=np.array(xk), pk=pk, c1=c1, c2=c2, amax=1)[0]\n",
    "            if alpha is None:\n",
    "                print(\"WARNING!!!-------------------------------------------------------------------\")\n",
    "                alpha_prev = p * alpha_prev\n",
    "                alpha = alpha_prev\n",
    "            #print(\"alpha = \", alpha)\n",
    "            #print(\"xk = \", xk)\n",
    "            #print(\"f = \", rosenbrock_fun(xk))\n",
    "            #print(\"k =\", k)\n",
    "            #xk_next = xk + alpha * pk\n",
    "            #print(\"found line search\\n\")\n",
    "\n",
    "        xk = xk_next\n",
    "        alpha = alpha_original\n",
    "        alpha_prev = alpha_original\n",
    "        k = k + 1\n",
    "\n",
    "        if save_xk:\n",
    "            xk_arr = np.append(xk_arr, [xk])\n",
    "\n",
    "    if save_xk:\n",
    "        return xk, k, xk_arr\n",
    "\n",
    "    print(\"Iteration #\" + str(k) + \", x\" + str(k) + \" = \" + str(xk))\n",
    "    print(\"||gradient|| = \" + str(np.linalg.norm(rosenbrock_gradient(xk))))\n",
    "    print(\"f = \" + str(rosenbrock_fun(xk)) + \"\\n\")\n",
    "\n",
    "    return xk, k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The steepest descent method applied to x0 = [1.2, 1.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***The first 6 iterations:*** \n",
      "\n",
      "Iteration #0, x0 = [1.2, 1.2]\n",
      "||gradient|| = 125.16932531574973\n",
      "f = 5.8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "find_local_minimum(x0=[1.2, 1.2], c1=1e-4, c2=0.9, alpha=1, p=0.5, tol=1e-8, print_num=3699, method=\"sd\", save_xk=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The steepest descent method applied to x0 = [-1.2, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_local_minimum(x0=[-1.2, 1], c1=1e-4, c2=0.9, alpha=1, p=0.5, tol=1e-8, print_num=True, method=\"sd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The newton method applied to x0 = [1.2, 1.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_local_minimum(x0=[1.2, 1.2], c1=1e-4, c2=0.9, alpha=1, p=0.5, tol=1e-8, print_num=True, method=\"newton\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The newton method applied to x0 = [-1.2, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_local_minimum(x0=[-1.2, 1], c1=1e-4, c2=0.9, alpha=1, p=0.5, tol=1e-8, print_num=True, method=\"newton\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
